# -*- coding: utf-8 -*-
"""Holiday package_purchase_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L1s011pilXAmoKV3lVjk84eW8ES0VyJy
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as snb
import warnings
warnings.filterwarnings('ignore')
# %matplotlib inline

df=pd.read_csv('/content/Travel.csv')

df.head()

# Data cleaning
df.info()



"""**Hnadling missing values**


1) Handling missing values

2)handling duplicates

3) Check datatype

4)understand dataset


"""

# missing value

df.isnull().sum()

# check all categorical data

df['Gender'].value_counts()

df['Gender']=df['Gender'].replace('Fe Male','Female')

df['TypeofContact'].value_counts()

df['Occupation'].value_counts()

df['ProductPitched'].value_counts()

df['MaritalStatus'].value_counts()

df['MaritalStatus	']=df['MaritalStatus'].replace('Single','Unmarried')

df['MaritalStatus'].value_counts()

df['Designation'].value_counts()

df.head()

df['MaritalStatus'].value_counts()

df['MaritalStatus']=df['MaritalStatus'].replace('Single','Unmarried')

df['MaritalStatus'].value_counts()

df['Gender'].value_counts()

df.head()

# checking missing values
## these are the featues with nan value

features_with_na=[features for features in df.columns if df[features].isnull().sum()>1]
for feature in features_with_na:
  print(feature,np.round(df[feature].isnull().mean()*100,5),' % missing values')

# statistic on numerical column

df[features_with_na].select_dtypes(exclude='object').describe()

"""**Imputing null values**


1. impute median for age
2.impute mode for contact
3.median for duration pich
4.mode for prefferedPropertystar
5.mode for numberoffollowup
6.median for number of trip
7.mode for number of children visiting
8.median for monthlyincome

"""

# Age

df['Age'].fillna(df['Age'].median(),inplace=True)

# TypeofContract

df['TypeofContact'].fillna(df['TypeofContact'].mode()[0], inplace=True)

# DurationOfPitch

df['DurationOfPitch'].fillna(df['DurationOfPitch'].mode()[0], inplace=True)

#NumberofTrips

df['NumberOfTrips'].fillna(df['NumberOfTrips'].median(),inplace=True)

#Numberoffollowup

df['NumberOfFollowups'].fillna(df['NumberOfFollowups'].mode()[0], inplace=True)

#prefferedPropertystar

df['PreferredPropertyStar'].fillna(df['PreferredPropertyStar'].mode()[0],inplace=True)

#MonthlyIncome

df['MonthlyIncome'].fillna(df['MonthlyIncome'].median(),inplace=True)

#number of children visiting

df['NumberOfChildrenVisiting'].fillna(df['NumberOfChildrenVisiting'].mode()[0],inplace=True)

df.drop('CustomerID',inplace=True,axis=1)

df.head()

## Featrure engineering

# adding two column
df['TotalVisiting'] = df['NumberOfChildrenVisiting'] + df['NumberOfPersonVisiting']

df.drop(['NumberOfChildrenVisiting','NumberOfPersonVisiting'],axis=1,inplace=True)

df.head()

# get all numerica feature
num_features=[col for col in df.columns if df[col].dtype!='object']
print("number of numerical feature:",len(num_features))

# get the categorical feature
cat_features=[col for col in df.columns if df[col].dtype=='object']
print("number of categorical feature:",len(cat_features))

# descret feature
discrete_feature=[col for col in num_features if len(df[col].unique())<25]
print("number of discrete feature:",len(discrete_feature))

# comtinous feature
continuous_feature=[col for col in num_features if col not in discrete_feature]
print("number of continuous feature:",len(continuous_feature))

# Train-test split
from sklearn.model_selection import train_test_split
X=df.drop('ProdTaken',axis=1)
y=df['ProdTaken']

X.head()

y.head()

y.value_counts()

# TRin test split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)

X_train.shape ,y_train.shape

# create coloumn transformer with 3 types of transformer
num_features=X.select_dtypes(exclude='object').columns
cat_features=X.select_dtypes(include='object').columns


from sklearn.preprocessing import OneHotEncoder,StandardScaler
from sklearn.compose import ColumnTransformer

numeric_transformer=StandardScaler()
oh_transformer=OneHotEncoder()

preprocessor=ColumnTransformer(
    [
     ('OneHotEncoder',oh_transformer,cat_features),
     ('StandardScaler',numeric_transformer,num_features)
    ]
)

preprocessor

# applying transformation in training dataset

X_train=preprocessor.fit_transform(X_train)

X_train

pd.DataFrame(X_train)

# apply transformation on test dats
X_test=preprocessor.transform(X_test)

X_test

# random forest classifier

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report, f1_score, precision_score, recall_score, roc_auc_score # Import f1_score

models={
    'Random Forest':RandomForestClassifier()
}

for i in range(len(list(models))):
  model=list(models.values())[i]
  model.fit(X_train,y_train)# train model

  # model prediction
y_train_pred=model.predict(X_train)
y_test_pred=model.predict(X_test)

# training set performace
model_train_accuracy=accuracy_score(y_train,y_train_pred)
model_train_f1=f1_score(y_train,y_train_pred,average='weighted')
model_train_precision=precision_score(y_train,y_train_pred)
model_train_recall=recall_score(y_train,y_train_pred)
mpdel_train_rocauc_score=roc_auc_score(y_train,y_train_pred)


# test performance
model_test_accuracy=accuracy_score(y_test,y_test_pred)
model_test_f1=f1_score(y_test,y_test_pred, average='weighted')
model_test_precision=precision_score(y_test,y_test_pred)
model_test_recall=recall_score(y_test,y_test_pred)
mpdel_test_rocauc_score=roc_auc_score(y_test,y_test_pred)

print(list(models.keys())[0])

print("Model performance for Training set")
print("-Accuracy: {:.4f}".format(model_train_accuracy))
print("-F1 Score: {:.4f}".format(model_train_f1))
print("-Precision: {:.4f}".format(model_train_precision))
print("-Recall: {:.4f}".format(model_train_recall))
print("-ROC AUC Score: {:.4f}".format(mpdel_train_rocauc_score))


print("----------------------------------")
print("Model performance for Test set")
print("-Accuracy: {:.4f}".format(model_test_accuracy))
print("-F1 Score: {:.4f}".format(model_test_f1))
print("-Precision: {:.4f}".format(model_test_precision))
print("-Recall: {:.4f}".format(model_test_recall))



print("="*35)
print('\n')

## Hyper parameter training

rf_param={"max_depth":[3,5,10,None],
          "min_samples_split":[2,5,10],
          "min_samples_leaf":[1,2,5],
          "n_estimators":[10,25,30,100,200]}

rf_param

# hyperparameter tunning

randomcv_model=[
    ('RF',RandomForestClassifier(),rf_param),

]

randomcv_model

from sklearn.model_selection import RandomizedSearchCV

model_params={}
for name,model,params in randomcv_model:
  rs_model=RandomizedSearchCV(model,
                              param_distributions=params,n_iter=100,cv=5,verbose=2)
  rs_model.fit(X_train,y_train)
  model_params[name]=rs_model.best_params_  # Use rs_model to access best parameters
  print(f"{name} model best params:{rs_model.best_params_}")

models={
    'Random Forest':RandomForestClassifier()
}

for i in range(len(list(models))):
  model=list(models.values())[i]
  model.fit(X_train,y_train)# train model

  # model prediction
y_train_pred=model.predict(X_train)
y_test_pred=model.predict(X_test)

# training set performace
model_train_accuracy=accuracy_score(y_train,y_train_pred)
model_train_f1=f1_score(y_train,y_train_pred,average='weighted')
model_train_precision=precision_score(y_train,y_train_pred)
model_train_recall=recall_score(y_train,y_train_pred)
mpdel_train_rocauc_score=roc_auc_score(y_train,y_train_pred)


# test performance
model_test_accuracy=accuracy_score(y_test,y_test_pred)
model_test_f1=f1_score(y_test,y_test_pred, average='weighted')
model_test_precision=precision_score(y_test,y_test_pred)
model_test_recall=recall_score(y_test,y_test_pred)
mpdel_test_rocauc_score=roc_auc_score(y_test,y_test_pred)

print(list(models.keys())[0])

print("Model performance for Training set")
print("-Accuracy: {:.4f}".format(model_train_accuracy))
print("-F1 Score: {:.4f}".format(model_train_f1))
print("-Precision: {:.4f}".format(model_train_precision))
print("-Recall: {:.4f}".format(model_train_recall))
print("-ROC AUC Score: {:.4f}".format(mpdel_train_rocauc_score))


print("----------------------------------")
print("Model performance for Test set")
print("-Accuracy: {:.4f}".format(model_test_accuracy))
print("-F1 Score: {:.4f}".format(model_test_f1))
print("-Precision: {:.4f}".format(model_test_precision))
print("-Recall: {:.4f}".format(model_test_recall))



print("="*35)
print('\n')